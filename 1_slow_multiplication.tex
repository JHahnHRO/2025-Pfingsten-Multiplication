% !TeX root = main.tex
% !TeX spellcheck = de_DE

\subsection{Das Problem}\label{problem-description}

\begin{remark}
    Konkret ist unser Problem folgendes:

    \medskip
    Gegeben zwei natürliche Zahlen $A,B$, dargestellt durch ihre $n$ Ziffern in Basis $g$
    \[A=\sum_{i=0}^{n-1} a_i g^i,\quad B=\sum_{i=0}^{n-1} b_i g^i\]
    mit $0\leq a_i,b_i<g$, berechne die Ziffern $c_0, c_1, \ldots, c_{2n-1}$ des Produkts $C:=A\cdot B$.

    \medskip
    Wir messen die Laufzeit von Multiplikationsalgorithmen in Abhängigkeit von $n$. Wir nehmen an, dass Operationen auf einzelnen Ziffern eine konstante Laufzeit haben.
\end{remark}

\begin{remark}
    Die Annahme, dass Operationen mit einzelnen Ziffern eine konstante Laufzeit haben, ist in der Theorie dadurch gerechtfertigt, dass man eine Additions- und Multiplikationstabelle vorberechnen kann. Dann hat jede Addition oder Multiplikation von zwei Ziffern die Laufzeit von \enquote{Schau einen Eintrag in eine $n\times n$ Tabelle nach}.

    So funktionierte das letztendlich auch in der Grundschule. Das Einmaleins haben wir auswendig gelernt und schriftliches Addieren und Multiplizieren von größeren Zahlen nutzt den \enquote{Lookup} im Gedächtnis.
\end{remark}

\begin{remark}
    In Computerhardware sind i.A.\ keine Lookup-Tabellen implementiert. Die offensichtliche Wahl der Basis wäre $g=2$. Die Addition und Multiplikation von einzelnen Bits sind einfach\footnote{Wie \enquote{einfach} es auch sein mag, wenige hundert Atome große Schaltkreise zu bauen...} direkt in Hardware zu implementieren, sodass kein Lookup nötig ist.

    \smallskip
    Etwas weniger offensichtlich ist es, auf \enquote{Wörtern} statt Bits zu arbeiten, d.h. $g=2^8$, $2^{16}$ (richtig alte Hardware), $2^{32}$ (alte Hardware und einige moderne Mobilgeräte) oder $2^{64}$ (moderne Hardware, insb. PCs) zu wählen. Manche Hardware unterstützt sogar noch höhere Wortgrößen.

    \smallskip
    Während Speicher für eine $256\times 256$ große Tabelle mit je $\SI{2}{\byte}$ großen Einträgen sicher noch verkraftbar wäre, ist eine $2^{32}\times 2^{32}$ Tabelle schon zu groß und eine $2^{64}\times 2^{64}$  Lookup-Tabelle völlig unrealistisch: $2^{64}$ viele $\SI{64}{\bit}$ große Einträge sind $\SI{1.476e8}{\tera\byte}$ und $2^{128}$ viele $\SI{128}{\bit}$ große Einträge sind $\approx \SI{5.445e27}{\tera\byte}$. Kein Datacenter der Welt hat diese Kapazität! Das ist ein Vielfaches der Gesamtgröße des Internets.

    \smallskip
    Also wird auch in diesen Fällen das Ergebnis einer einzelnen Multiplikation mit einer speziellen Instruktion direkt von der Hardware berechnet. Die Schaltkreise sind allerdings schon deutlich komplexer. Und in der Tat sind die Schaltkreise für größere Wörter i.d.R.\ zusammengesetzt aus Schaltkreisen, die Addition und Multiplikation kleinerer Wörter implementieren, d.h.\ es wird ein passender Multiplikationsalgorithmus in Hardware implementiert.

    \smallskip
    Solange alle Multiplikation von zwei Wörtern stets durch den gleichen Schaltkreis ermittelt wird, kann aber ebenfalls davon ausgegangen werden, dass die Laufzeitkosten konstant sind.
\end{remark}

\begin{remark}
    Gewisse Operationen zählen wir nicht zur Laufzeit des Algorithmus:

    \smallskip
    Wir werden keine Additionen zählen, die Schleifenvariablen inkrementieren. Die sind Teil des Konstrukts \enquote{Schleife}, nicht unseres spezifischen Algorithmus. Und wie die Schleifen tatsächlich ausgeführt werden, ist außerdem zu schwer vorhersagbar, um konkrete Anzahlen an Additionen vorhersagen zu können (Stichwort \enquote{loop-tiling}, \enquote{loop-unrolling}, \enquote{auto-vectorization}, \enquote{SIMD}, \ldots)

    \medskip
    Wir werden uns außerdem keine Gedanken um Speicherzugriffe machen. Speicher ist in unserer Analyse kostenlos.

    Auch hier ist die Realität wieder sehr anders; die echte Laufzeit echten Codes auf echter Hardware ist sehr wohl davon abhängig, wie häufig im Hauptspeicher gelesen oder geschrieben wird, ob Caches effizient genutzt werden können etc. Es gibt auch Theorie für diese Aspekte (Stichwort \enquote{cache-friendliness}, \enquote{cache-oblivious algorithms} etc.), aber wir werden das in diesem Kurs nicht untersuchen.
\end{remark}

\subsection{Der erste Algorithmus}\label{slow_multiplication:first_algorithm}

\begin{remark}
    Der offensichtliche Algorithmus für Multiplikation beliebiger natürlicher Zahlen ergibt sich direkt durch Anwendung des Distributivgesetzes:
    \[\left(\sum_{i=0}^{n-1}\ a_i g^i \right)\cdot\left(\sum_{j=0}^{n-1} b_j g^j\right) = \sum_{i,j} (a_i b_j) g^{i+j} = \sum_{k=0}^{2n-2} \left(\sum_{i+j=k}a_i b_j\right)g^k\]
    Indem wir alle $n^2$ Produkte $a_i b_j$ berechnen und geeignet aufsummieren, erhalten wir das Produkt $A\cdot B$.

    \medskip
    In der Grundschule lernt man meist ein tabellarisches Verfahren, das schematisch in etwa so abläuft, dass in jeder Zeile das Produkt einer einzelnen Ziffer $a_i$ von $A$ mit $B$ ausgerechnet wird, um $i$ viele Dezimalstellen verschoben wird (was die Multiplikation mit $10^i$ darstellt), sowie am Ende alle Zeilen aufsummiert werden.

    \smallskip
    In jeder Zeile werden dafür die Produkte $a_i\cdot b_j$ ausgerechnet. Grob vereinfacht würde das also so aussehen:

    \[\begin{array}{c c c c c c c c}
          a_2 & a_1 & a_0     & \cdot   & b_3     & b_2     & b_1     & b_0     \\
          \hline
          &     &         &         & a_0 b_3 & a_0 b_2 & a_0 b_1 & a_0 b_0 \\
          &     &         & a_1 b_3 & a_1 b_2 & a_1 b_1 & a_1 b_0 & 0       \\
          &     & a_2 b_3 & a_2 b_2 & a_2 b_1 & a_2 b_0 & 0       & 0       \\
          \hline
          & c_6 & c_5     & c_4     & c_3     & c_2     & c_1     & c_0
    \end{array}\]

    Natürlich sind die Produkte $a_i\cdot b_j$ der einzelnen Ziffern i.A.\ größer als eine Ziffer, sodass man in jeder Zeile und von einer Zeile zur nächsten jeweils die Überträge beachten muss.
\end{remark}

\begin{algorithm}[Schriftliches Multiplizieren]
    \label{alg:slow_multiplication}
    Wir benutzen die Schulmethode direkt. Einziger Unterschied ist, dass wir ein laufendes Zwischenergebnis pflegen und dies kontinuierlich updaten, anstatt $n$ viele Zwischenergebnisse zu speichern und erst am Ende alle aufzuaddieren.

    %! suppress = MissingLabel
    \begin{lstlisting}
    List<Digit> multiply(List<Digit> a, List<Digit> b){
      // initialize with a.length + b.length zeros
      List<Digit> c := new List<Digit>(a.length+b.length);

      for(i := 0; i < a.length; i++){
        Digit z := 0;

        for(j := 0; j < b.length; j++){
          // p := c[i+j] + a_i * b_j + z is at most a 2-digit number !!
          Digits {p_1,p_0} := c[i+j] + a[i] * b[j] + z;

          c[i+j] := p_0;
          z      := p_1;
        }
        c[i + b.length] := z;
      }

      return c;
    }
    \end{lstlisting}
\end{algorithm}

\begin{proposition}[Korrektheit]
    Der Algorithmus~\ref{alg:slow_multiplication} ist korrekt. Er benötigt $n^2$ Multiplikation, $O(n^2)$ Additionen und benötigt $2n+O(1)$ Speicher.
\end{proposition}
\begin{proof}
    Das meiste ist relativ klar. Wir beginnen, indem wir $C$ mit den Ziffern $000\ldots 000$ initialisieren. Dann addieren wir für jedes $i$ die Zahl $a_i g^i\cdot B$ zu $C$, indem wir $a_i \cdot B$ berechnen und auf die Ziffern $\ldots c_{i+2}c_{i+1}c_i$ addieren, und speichern das Ergebnis wieder in $C$.

    \smallskip
    Wir addieren in jeder Iteration der inneren Schleife die 1er-Ziffer des Produkts $p:=a_i \cdot b_j$ an die korrekte Stelle $c_{i+j}$. Wir addieren außerdem den Übertrag der letzten Iteration an diese Stelle und merken uns den Übertrag, den wir dabei erhalten, für die nächste Iteration.

    \smallskip
    Damit das funktioniert, müssen wir die mit \lstinline{!!} markierte Behauptung beweisen:

    Das Produkt zweier Ziffern $a_i,b_j\leq g-1$ ist höchstens $(g-1)^2=g^2-2g+1$ und die Addition von zwei weiteren Ziffern $c,z\leq g-1$ führt zu einem Maximum von $g^2-1$, was echt kleiner $g^2$ ist, also wirklich maximal zwei Ziffern hat. Der Übertrag $z$ ist also auch in der nächsten Iteration maximal eine Ziffer groß.
\end{proof}

\begin{remark}
    Wie wir eben gezeigt haben, ist es möglich, einen Schaltkreis zu bauen, der vier Wörter $a,b,c,z$ als Input nimmt, und in einer einzigen Operation das 2-Wort breite Ergebnis $ab+c+z$ berechnet.

    \smallskip
    Es ist von der konkreten Prozessorarchitektur abhängig, ob es tatsächlich eine solche Instruktion gibt. Wenn es sie gibt, sind überhaupt keine Additionsinstruktionen zusätzlich zu den Multiplikationen für unseren Algorithmus nötig.

    \smallskip
    Wenn es diesen Schaltkreis nicht gibt, dann gibt es oft trotzdem $(a,b,c)\mapsto ab+c$, womit wir nur noch eine einzige Addition zusätzlich zur Multiplikation benötigen.
\end{remark}

\begin{remark}
    Der Algorithmus ist offenbar nicht davon abhängig, dass $A$ und $B$ die gleiche Anzahl an Ziffern haben. Etwas präziser könnten wir die Laufzeiten also als $O(nm)$ ausdrücken, wobei $n$ und $m$ die Längen der beiden Zahlen sind.

    \smallskip
    Darin enthalten ist die Teilaussage, dass die Multiplikation mit einer kleinen Konstanten in $O(n)$ realisierbar ist. Das merken wir uns für später, weil wir es verwenden werden.
\end{remark}

\begin{remark}
    Man beachte auch, dass die asymptotische Laufzeit nicht von der Wahl von $g$ abhängig ist. Wenn wir eine andere Basis $g'$ wählen, dann sind $A$ und $B$ Zahlen mit $\approx \frac{\log(g)}{\log(g')}n$ Ziffern. Die Basis zu ändern, ändert also die Laufzeit nur um einen konstanten Faktor.
\end{remark}


\begin{remark}
    Es ist nicht einmal notwendig, die beiden Inputs $A$ und $B$ bzgl. derselben Basis auszudrücken. In der Tat ist das genau das Prinzip hinter der \enquote{ägyptischen Multiplikation}, auch \enquote{(russische) Bauernmultiplikation} genannt:

    Drückt man eine der beiden Zahlen in Basis $2$ aus, etwa $A$, so reduziert sich das Problem darauf, aus $B$ die Zahlen $2B$, $4B$, $8B$, \ldots zu berechnen, da die Multiplikationen $a_i\cdot B$ entweder $0$ oder $B$ ist (denn $a_i$ ist ja $0$ oder $1$ in Basis $2$), also keinen zusätzlichen Rechenaufwand verursachen:

    \[(a_{n-1} 2^{n-1} + \ldots + a_2 2^2 + a_1 2 + a_0) \cdot B = \sum_{\substack{0\leq i\leq n-1 \\ a_i = 1}} 2^i B\]

    Man kann also mit $B$ beginnen und die Zahl immer wieder zu sich selbst addieren, um $2B$, $4B$, \ldots zu erhalten. Und während dessen, jedes Mal, wenn das passende Bit in $A$ gesetzt ist, eine weitere Addition auf das Zwischenergebnis durchführen.
\end{remark}

\begin{algorithm}[Double-and-add]
    \label{alg:double-and-add}
    Input: $A$ in Binärdarstellung $a_{n-1}2^{n-1}+\ldots a_{2}2^2 +a_1 2 + a_0$, $B$ in beliebiger Basis $b_{m-1} g^{m-1} + \ldots b_2 g^2 + b_1 g+b_0$.

    Output: Das Produkt $C=A\cdot B$ in Basis $g$.

    Pseudocode:
    %! suppress = MissingLabel
    \begin{lstlisting}
    List<Digit> multiply(List<Bit> a, List<Digit> b){
      // calculate the number of base-g-digits in A
      n := floor( a.length * log(2)/log(g) ) + 1;
      c := new List<Digit>(n + b.length);

      // stores 2^i*b
      b2 := b;
      for(i := 0; i<a.length; i++){
        if(a[i] == 1){
          c := add(c, b2);
        }
        b2 := add(b2, b2);
      }

      return c;
    }
    \end{lstlisting}
\end{algorithm}

\begin{proposition}[Korrektheit und Laufzeit]
    \label{alg:double-and-add:correctness}
    Der Algorithmus ist korrekt, hat Laufzeit $O(n^2)$ (alles Additionen) und Speicherbedarf $4n+O(1)$.
\end{proposition}
\begin{proof}
    Übung.
\end{proof}

\begin{remark}
    Da man in der Praxis eben nicht mit beliebigen $g$, sondern mit Zweierpotenzen $g=2^k$ arbeitet, ist für die Berechnung von $\frac{\log(2)}{\log(g)}$ keine Fließkomma-Arithmetik nötig, da sich alles zu $\frac{1}{k}$ kürzt. Die Länge von $A$ in bits ist typischerweise klein genug, damit die Division \texttt{a.length / k} in einer einzigen Hardware-Instruktion berechnet werden kann.

    \smallskip
    Mehr noch: Wenn $g$ selbst eine Zweierpotenz ist, dann besteht eigentlich kein Unterschied zwischen \texttt{List<Bit>} und \texttt{List<Digit>} und es ist überhaupt keine zusätzliche Aktion nötig.
\end{remark}